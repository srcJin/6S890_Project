{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Creation\n",
    "\n",
    "本文档概述了创建新环境以及 PettingZoo 中为创建新环境而设计的相关有用包装器、实用程序和测试。\n",
    "\n",
    "我们将逐步创建一个简单的剪刀石头布环境，并提供AEC和并行环境的示例代码。代理环境循环（“AEC”）\n",
    "\n",
    "请参阅我们的自定义环境教程，了解创建自定义环境的完整演练，包括复杂的环境逻辑和非法操作屏蔽。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Custom Environment\n",
    "\n",
    "这是一个经过仔细注释的 PettingZoo 石头剪刀布环境版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def rps_env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class raw_env(AECEnv):\n",
    "    \"\"\"\n",
    "    The metadata holds environment constants. From gymnasium, we inherit the \"render_modes\",\n",
    "    metadata which specifies which modes can be put into the render() method.\n",
    "    At least human mode should be supported.\n",
    "    The \"name\" metadata allows the environment to be pretty printed.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment arguments and\n",
    "         should define the following attributes:\n",
    "        - possible_agents\n",
    "        - render_mode\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "\n",
    "        # optional: a mapping between agent name and ID\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "        # optional: we can define the observation and action spaces here as attributes to be used in their corresponding methods\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Discrete(4) for agent in self.possible_agents\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"\n",
    "        Observe should return the observation of the specified agent. This function\n",
    "        should return a sane observation (though not necessarily the most up to date possible)\n",
    "        at any time after reset() is called.\n",
    "        \"\"\"\n",
    "        # observation of one agent is the previous state of the other\n",
    "        return np.array(self.observations[agent])\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the following attributes\n",
    "        - agents\n",
    "        - rewards\n",
    "        - _cumulative_rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection\n",
    "        And must set up the environment so that render(), step(), and observe()\n",
    "        can be called without issues.\n",
    "        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: NONE for agent in self.agents}\n",
    "        self.observations = {agent: NONE for agent in self.agents}\n",
    "        self.num_moves = 0\n",
    "        \"\"\"\n",
    "        Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \"\"\"\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for the current agent (specified by\n",
    "        agent_selection) and needs to update\n",
    "        - rewards\n",
    "        - _cumulative_rewards (accumulating the rewards)\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection (to the next agent)\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            # handles stepping an agent which is already dead\n",
    "            # accepts a None action for the one agent, and moves the agent_selection to\n",
    "            # the next dead agent,  or if there are no more dead agents, to the next live agent\n",
    "            self._was_dead_step(action)\n",
    "            return\n",
    "\n",
    "        agent = self.agent_selection\n",
    "\n",
    "        # the agent which stepped last had its _cumulative_rewards accounted for\n",
    "        # (because it was returned by last()), so the _cumulative_rewards for this\n",
    "        # agent should start again at 0\n",
    "        self._cumulative_rewards[agent] = 0\n",
    "\n",
    "        # stores action of current agent\n",
    "        self.state[self.agent_selection] = action\n",
    "\n",
    "        # collect reward if it is the last agent to act\n",
    "        if self._agent_selector.is_last():\n",
    "            # rewards for all agents are placed in the .rewards dictionary\n",
    "            self.rewards[self.agents[0]], self.rewards[self.agents[1]] = REWARD_MAP[\n",
    "                (self.state[self.agents[0]], self.state[self.agents[1]])\n",
    "            ]\n",
    "\n",
    "            self.num_moves += 1\n",
    "            # The truncations dictionary must be updated for all players.\n",
    "            self.truncations = {\n",
    "                agent: self.num_moves >= NUM_ITERS for agent in self.agents\n",
    "            }\n",
    "\n",
    "            # observe the current state\n",
    "            for i in self.agents:\n",
    "                self.observations[i] = self.state[\n",
    "                    self.agents[1 - self.agent_name_mapping[i]]\n",
    "                ]\n",
    "        else:\n",
    "            # necessary so that observe() returns a reasonable observation at all times.\n",
    "            self.state[self.agents[1 - self.agent_name_mapping[agent]]] = NONE\n",
    "            # no rewards are allocated until both players give an action\n",
    "            self._clear_rewards()\n",
    "\n",
    "        # selects the next agent.\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        # Adds .rewards to ._cumulative_rewards\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要与自定义 AEC 环境交互，请使用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n"
     ]
    }
   ],
   "source": [
    "env = rps_env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Custom Parallel Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def raw_env(render_mode=None):\n",
    "    \"\"\"\n",
    "    To support the AEC API, the raw_env() function just uses the from_parallel\n",
    "    function to convert from a ParallelEnv to an AEC env\n",
    "    \"\"\"\n",
    "    env = parallel_env(render_mode=render_mode)\n",
    "    env = parallel_to_aec(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class parallel_env(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment arguments and should define the following attributes:\n",
    "        - possible_agents\n",
    "        - render_mode\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "\n",
    "        # optional: a mapping between agent name and ID\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the `agents` attribute and must set up the\n",
    "        environment so that render(), and step() can be called without issues.\n",
    "        Here it initializes the `num_moves` variable which counts the number of\n",
    "        hands that are played.\n",
    "        Returns the observations for each agent\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.num_moves = 0\n",
    "        observations = {agent: NONE for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        self.state = observations\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for each agent and should return the\n",
    "        - observations\n",
    "        - rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        dicts where each dict looks like {agent_1: item_1, agent_2: item_2}\n",
    "        \"\"\"\n",
    "        # If a user passes in actions with no agents, then just return empty observations, etc.\n",
    "        if not actions:\n",
    "            self.agents = []\n",
    "            return {}, {}, {}, {}, {}\n",
    "\n",
    "        # rewards for all agents are placed in the rewards dictionary to be returned\n",
    "        rewards = {}\n",
    "        rewards[self.agents[0]], rewards[self.agents[1]] = REWARD_MAP[\n",
    "            (actions[self.agents[0]], actions[self.agents[1]])\n",
    "        ]\n",
    "\n",
    "        terminations = {agent: False for agent in self.agents}\n",
    "\n",
    "        self.num_moves += 1\n",
    "        env_truncation = self.num_moves >= NUM_ITERS\n",
    "        truncations = {agent: env_truncation for agent in self.agents}\n",
    "\n",
    "        # current observation is just the other player's most recent action\n",
    "        observations = {\n",
    "            self.agents[i]: int(actions[self.agents[1 - i]])\n",
    "            for i in range(len(self.agents))\n",
    "        }\n",
    "        self.state = observations\n",
    "\n",
    "        # typically there won't be any information in the infos, but there must\n",
    "        # still be an entry for each agent\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        if env_truncation:\n",
    "            self.agents = []\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observations, rewards, terminations, truncations, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要与自定义并行环境交互，请使用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Game over\n"
     ]
    }
   ],
   "source": [
    "env = parallel_env(render_mode=\"human\")\n",
    "observations, infos = env.reset()\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Wrappers\n",
    "包装器是一种环境转换，它将环境作为输入，并输出与输入环境类似的新环境，但应用了一些转换或验证。 PettingZoo 提供了用于在 AEC API 和并行 API 之间来回转换环境的包装器，以及一组提供输入验证和其他方便的可重用逻辑的简单实用程序包装器。 PettingZoo 还通过 SuperSuit 配套包 ( pip install supersuit ) 包含包装器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from pettingzoo.utils import ClipOutOfBoundsWrapper\n",
    "\n",
    "env = pistonball_v6.env()\n",
    "wrapped_env = ClipOutOfBoundsWrapper(env)\n",
    "# Wrapped environments must be reset before use\n",
    "wrapped_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developer Utils\n",
    "utils 目录包含一些有助于调试环境的函数。这些都记录在 API 文档中。\n",
    "utils 目录还包含一些仅对开发新环境有帮助的类。这些记录如下。\n",
    "\n",
    "## Agent selector\n",
    "\n",
    "agent_selector类循环遍历代理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils import agent_selector\n",
    "agents = [\"agent_1\", \"agent_2\", \"agent_3\"]\n",
    "selector = agent_selector(agents)\n",
    "agent_selection = selector.reset()\n",
    "# agent_selection will be \"agent_1\"\n",
    "for i in range(100):\n",
    "    agent_selection = selector.next()\n",
    "    # will select \"agent_2\", \"agent_3\", \"agent_1\", \"agent_2\", \"agent_3\", ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Environments\n",
    "\n",
    "PettingZoo 通过了多项环境合规性测试。如果您要添加新环境，我们鼓励您在自己的环境中运行这些测试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Test\n",
    "PettingZoo 的 API 有许多功能和要求。为了确保您的环境与 API 一致，我们有 api_test。下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.env()\n",
    "api_test(env, num_cycles=1000, verbose_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "您只需将环境传递给测试即可。该测试将asssert或给出有关 API 问题的其他错误，如果通过则将正常返回。\n",
    "\n",
    "可选参数是：\n",
    "\n",
    "num_cycles ：运行环境这么多周期并检查输出是否与 API 一致。\n",
    "\n",
    "verbose_progress ：打印出消息以指示测试部分完成。对于调试环境很有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行 API 测试\n",
    "适用于并行环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import parallel_api_test\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.parallel_env()\n",
    "parallel_api_test(env, num_cycles=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Test\n",
    "为了拥有一个利用随机性的正确可重现环境，您需要能够通过为定义随机行为的随机数生成器设置种子来使其在评估过程中具有确定性。种子测试检查使用常量调用seed()方法实际上使环境具有确定性。\n",
    "种子测试采用创建 pettingzoo 环境的函数。例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import seed_test, parallel_seed_test\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env_fn = pistonball_v6.env\n",
    "seed_test(env_fn, num_cycles=10)\n",
    "\n",
    "# or for parallel environments\n",
    "parallel_env_fn = pistonball_v6.parallel_env\n",
    "parallel_seed_test(parallel_env_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在内部，有两个单独的测试。\n",
    "环境播种后，两个单独的环境是否会给出相同的结果？\n",
    "在调用seed()然后调用reset()之后，单个环境是否会给出相同的结果？\n",
    "\n",
    "第一个可选参数num_cycles指示环境将运行多长时间来检查确定性。有些环境仅在初始化后很长时间才通过测试。\n",
    "\n",
    "第二个可选参数test_kept_state允许用户禁用第二个测试。一些基于物理的环境未能通过此测试，因为缓存等造成的差异几乎无法检测到，而这些差异并不重要。\n",
    "\n",
    "\n",
    "## Max Cycles Test 最大循环测试\n",
    "最大周期测试测试max_cycles环境参数是否存在以及生成的环境实际运行了正确的周期数。\n",
    "\n",
    "如果您的环境不采用max_cycles参数，则不应运行此测试。该测试存在的原因是在实现max_cycles时可能会出现许多相差一的错误。测试用法示例如下：\n",
    "\n",
    "from pettingzoo.test import max_cycles_test\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "max_cycles_test(pistonball_v6)\n",
    "\n",
    "\n",
    "## Render Test\n",
    "\n",
    "渲染测试检查渲染\n",
    "\n",
    " 1) 不会崩溃，2) 在给定模式时生成正确类型的输出（仅支持'human' 、 'ansi'和'rgb_array'模式）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import render_test\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env_func = pistonball_v6.env\n",
    "render_test(env_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "渲染测试方法采用可选参数custom_tests ，该参数允许在非标准模式下进行附加测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom_tests = {\n",
    "    \"svg\": lambda render_result: return isinstance(render_result, str)\n",
    "}\n",
    "render_test(env, custom_tests=custom_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark Test 性能基准测试\n",
    "\n",
    "为了确保我们不会出现性能下降，我们进行了性能基准测试。该测试只是打印出环境在 5 秒内执行的步数和循环数。该测试需要手动检查其输出：\n",
    "\n",
    "from pettingzoo.test import performance_benchmark\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.env()\n",
    "performance_benchmark(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Observation Test\n",
    "\n",
    "保存观察测试是通过图形观察来目视检查游戏的观察结果，以确保它们符合预期。\n",
    "我们发现观察是环境中错误的巨大来源，因此最好在可能的情况下手动检查它们。\n",
    "该测试只是试图保存所有代理的观察结果。如果失败，它只会打印一条警告。需要目视检查输出的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import test_save_obs\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.env()\n",
    "test_save_obs(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
