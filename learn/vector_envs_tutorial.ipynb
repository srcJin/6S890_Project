{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training A2C with Vector Envs and Domain Randomization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "If you encounter an RuntimeError like the following comment raised on multiprocessing/spawn.py, wrap up the code from ``gym.vector.make=`` or ``gym.vector.AsyncVectorEnv`` to the end of the code by ``if__name__ == '__main__'``.\n",
    "\n",
    "``An attempt has been made to start a new process before the current process has finished its bootstrapping phase.``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "在本教程中，您将学习如何使用矢量化环境来训练 Advantage Actor-Critic 代理。我们将使用 A2C，它是 A3C 算法的同步版本 [1]。\n",
    "\n",
    "矢量化环境 [3] 允许同一环境的多个实例并行运行（在多个 CPU 上），有助于实现更快、更稳健的训练。这可以显着减少方差，从而加快训练速度。\n",
    "\n",
    "我们将从头开始实现一个 Advantage Actor-Critic，看看如何将批量状态输入到网络中以获得动作向量（每个环境一个动作），并计算小批量转换中 Actor 和 Critic 的损失。每个小批量包含一个采样阶段的转换： n_steps_per_update步骤在n_envs环境中并行执行（将两者相乘以获得小批量中的转换数量）。在每个采样阶段之后，计算损失并执行一个梯度步骤。为了计算优势，我们将使用广义优势估计（GAE）方法[2]，该方法平衡优势估计的方差和偏差之间的权衡。\n",
    "\n",
    "A2C 代理类使用输入状态的特征数量、代理可以采取的操作数量、学习率以及并行运行以收集经验的环境数量进行初始化。定义了参与者和批评者网络，并初始化了它们各自的优化器。网络的前向传递接受批量状态向量并返回状态值张量和动作逻辑张量。 select_action 方法返回所选操作的元组、这些操作的日志概率以及每个操作的状态值。此外，它还返回策略分布的熵，稍后从损失中减去该熵（使用权重因子ent_coef ）以鼓励探索。\n",
    "\n",
    "get_losses 函数计算参与者网络和评论家网络的损失（使用 GAE），然后使用 update_parameters 函数进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic (A2C)\n",
    "\n",
    "Actor-Critic 结合了基于价值和基于策略的方法的元素。在 A2C 中，代理有两个独立的神经网络：一个估计状态值函数的批评者网络，以及一个输出所有动作的分类概率分布的对数的参与者网络。训练批评家网络以最小化预测状态值与代理收到的实际回报之间的均方误差（这相当于最小化优势平方，因为动作的优势是回报与状态之间的差值） -value：A(s,a) = Q(s,a) - V(s)。演员网络经过训练，通过根据批评者网络选择具有高期望值的动作来最大化期望回报。\n",
    "\n",
    "本教程的重点不会是 A2C 本身的细节。相反，本教程将重点介绍如何使用矢量化环境和域随机化来加速 A2C（和其他强化学习算法）的训练过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    \"\"\"\n",
    "    (Synchronous) Advantage Actor-Critic agent class\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features of the input state.\n",
    "        n_actions: The number of actions the agent can take.\n",
    "        device: The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,\n",
    "                for this code CPU is totally fine).\n",
    "        critic_lr: The learning rate for the critic network (should usually be larger than the actor_lr).\n",
    "        actor_lr: The learning rate for the actor network.\n",
    "        n_envs: The number of environments that run in parallel (on multiple CPUs) to collect experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_actions: int,\n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the actor and critic networks and their respective optimizers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        critic_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),  # estimate V(s)\n",
    "        ]\n",
    "\n",
    "        actor_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                32, n_actions\n",
    "            ),  # estimate action logits (will be fed into a softmax later)\n",
    "        ]\n",
    "\n",
    "        # define actor and critic networks\n",
    "        self.critic = nn.Sequential(*critic_layers).to(self.device)\n",
    "        self.actor = nn.Sequential(*actor_layers).to(self.device)\n",
    "\n",
    "        # define optimizers for actor and critic\n",
    "        self.critic_optim = optim.RMSprop(self.critic.parameters(), lr=critic_lr)\n",
    "        self.actor_optim = optim.RMSprop(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the networks.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            state_values: A tensor with the state values, with shape [n_envs,].\n",
    "            action_logits_vec: A tensor with the action logits, with shape [n_envs, n_actions].\n",
    "        \"\"\"\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "        state_values = self.critic(x)  # shape: [n_envs,]\n",
    "        action_logits_vec = self.actor(x)  # shape: [n_envs, n_actions]\n",
    "        return (state_values, action_logits_vec)\n",
    "\n",
    "    def select_action(\n",
    "        self, x: np.ndarray\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple of the chosen actions and the log-probs of those actions.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            actions: A tensor with the actions, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions, with shape [n_steps_per_update, n_envs].\n",
    "            state_values: A tensor with the state values, with shape [n_steps_per_update, n_envs].\n",
    "        \"\"\"\n",
    "        state_values, action_logits = self.forward(x)\n",
    "        action_pd = torch.distributions.Categorical(\n",
    "            logits=action_logits\n",
    "        )  # implicitly uses softmax\n",
    "        actions = action_pd.sample()\n",
    "        action_log_probs = action_pd.log_prob(actions)\n",
    "        entropy = action_pd.entropy()\n",
    "        return (actions, action_log_probs, state_values, entropy)\n",
    "\n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards: torch.Tensor,\n",
    "        action_log_probs: torch.Tensor,\n",
    "        value_preds: torch.Tensor,\n",
    "        entropy: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        gamma: float,\n",
    "        lam: float,\n",
    "        ent_coef: float,\n",
    "        device: torch.device,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic\n",
    "        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).\n",
    "\n",
    "        Args:\n",
    "            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            gamma: The discount factor.\n",
    "            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,\n",
    "                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased\n",
    "                                          because the estimates are generated by a Neural Net).\n",
    "            device: The device to run the computations on (e.g. CPU or GPU).\n",
    "\n",
    "        Returns:\n",
    "            critic_loss: The critic loss for the minibatch.\n",
    "            actor_loss: The actor loss for the minibatch.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        advantages = torch.zeros(T, self.n_envs, device=device)\n",
    "\n",
    "        # compute the advantages using GAE\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T - 1)):\n",
    "            td_error = (\n",
    "                rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]\n",
    "            )\n",
    "            gae = td_error + gamma * lam * masks[t] * gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "        # calculate the loss of the minibatch for actor and critic\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # give a bonus for higher entropy to encourage exploration\n",
    "        actor_loss = (\n",
    "            -(advantages.detach() * action_log_probs).mean() - ent_coef * entropy.mean()\n",
    "        )\n",
    "        return (critic_loss, actor_loss)\n",
    "\n",
    "    def update_parameters(\n",
    "        self, critic_loss: torch.Tensor, actor_loss: torch.Tensor\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the actor and critic networks.\n",
    "\n",
    "        Args:\n",
    "            critic_loss: The critic loss.\n",
    "            actor_loss: The actor loss.\n",
    "        \"\"\"\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Vectorized Environments\n",
    "\n",
    "当您计算两个神经网络仅在一个时期内的损失时，它可能会有很高的方差。使用矢量化环境，我们可以并行使用n_envs ，从而达到线性加速（这意味着理论上，我们收集样本的速度要快n_envs倍），我们可以用它来计算当前策略和批评者网络的损失。当我们使用更多样本来计算损失时，它将具有更低的方差，因此可以更快地学习。\n",
    "\n",
    "A2C 是一种同步方法，这意味着对网络的参数更新是确定性地发生的（在每个采样阶段之后），但我们仍然可以利用异步向量环境来生成多个进程以进行并行环境执行。\n",
    "\n",
    "创建矢量环境的最简单方法是调用gym.vector.make ，它创建同一环境的多个实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gymnasium.vector import AsyncVectorEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "# Function to create individual environments\n",
    "def make_env():\n",
    "    return gym.make(\"LunarLander-v3\", max_episode_steps=600)\n",
    "\n",
    "# Create vectorized environments\n",
    "num_envs = 3\n",
    "envs = AsyncVectorEnv([make_env for _ in range(num_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Randomization\n",
    "\n",
    "如果我们想要随机化训练环境以获得更鲁棒的代理（可以处理环境的不同参数化，因此可能具有更高程度的泛化），我们可以手动设置所需的参数或使用伪随机数生成器来生成它们。\n",
    "\n",
    "手动设置 3 个具有不同参数的并行“LunarLander-v3”环境：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v3\",\n",
    "            gravity=-10.0,\n",
    "            enable_wind=True,\n",
    "            wind_power=15.0,\n",
    "            turbulence_power=1.5,\n",
    "            max_episode_steps=600,\n",
    "        ),\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v3\",\n",
    "            gravity=-9.8,\n",
    "            enable_wind=True,\n",
    "            wind_power=10.0,\n",
    "            turbulence_power=1.3,\n",
    "            max_episode_steps=600,\n",
    "        ),\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v3\", gravity=-7.0, enable_wind=False, max_episode_steps=600\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "随机生成 3 个并行“LunarLander-v3”环境的参数，使用np.clip保留在推荐的参数空间中：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        lambda: gym.make(\n",
    "            \"LunarLander-v3\",\n",
    "            gravity=np.clip(\n",
    "                np.random.normal(loc=-10.0, scale=1.0), a_min=-11.99, a_max=-0.01\n",
    "            ),\n",
    "            enable_wind=np.random.choice([True, False]),\n",
    "            wind_power=np.clip(\n",
    "                np.random.normal(loc=15.0, scale=1.0), a_min=0.01, a_max=19.99\n",
    "            ),\n",
    "            turbulence_power=np.clip(\n",
    "                np.random.normal(loc=1.5, scale=0.5), a_min=0.01, a_max=1.99\n",
    "            ),\n",
    "            max_episode_steps=600,\n",
    "        )\n",
    "        for i in range(3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "在这里，我们使用正态分布，以环境的标准参数化作为平均值和任意标准偏差（尺度）。根据问题的不同，您可以尝试更高的方差并使用不同的分布。\n",
    "\n",
    "如果您在整个训练时间内在相同的n_envs环境上进行训练，并且n_envs的数字相对较低（与环境的复杂程度成正比），则您可能仍然会对您选择的特定参数化产生一些过度拟合。为了缓解这种情况，您可以选择大量随机参数化环境，或者每隔几个采样阶段重新创建环境以生成一组新的伪随机参数。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# environment hyperparams\n",
    "n_envs = 10\n",
    "n_updates = 1000\n",
    "n_steps_per_update = 128\n",
    "randomize_domain = False\n",
    "\n",
    "# agent hyperparams\n",
    "gamma = 0.999\n",
    "lam = 0.95  # hyperparameter for GAE\n",
    "ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "\n",
    "# Note: the actor has a slower learning rate so that the value targets become\n",
    "# more stationary and are theirfore easier to estimate for the critic\n",
    "\n",
    "# environment setup\n",
    "if randomize_domain:\n",
    "    envs = AsyncVectorEnv(\n",
    "        [\n",
    "            lambda: gym.make(\n",
    "                \"LunarLander-v2\",  # \"v2\" is the correct version for LunarLander\n",
    "                gravity=np.clip(\n",
    "                    np.random.normal(loc=-10.0, scale=1.0), a_min=-11.99, a_max=-0.01\n",
    "                ),\n",
    "                enable_wind=np.random.choice([True, False]),\n",
    "                wind_power=np.clip(\n",
    "                    np.random.normal(loc=15.0, scale=1.0), a_min=0.01, a_max=19.99\n",
    "                ),\n",
    "                turbulence_power=np.clip(\n",
    "                    np.random.normal(loc=1.5, scale=0.5), a_min=0.01, a_max=1.99\n",
    "                ),\n",
    "                max_episode_steps=600,\n",
    "            )\n",
    "            for _ in range(n_envs)\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    envs = AsyncVectorEnv(\n",
    "        [\n",
    "            lambda: gym.make(\"LunarLander-v3\", max_episode_steps=600)\n",
    "            for _ in range(n_envs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.n\n",
    "\n",
    "# set the device\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "# init the agent\n",
    "agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the A2C Agent\n",
    "对于我们的训练循环，我们使用RecordEpisodeStatistics包装器来记录episode lengths和returns，并且我们还保存损失和熵以在代理完成训练后绘制它们。\n",
    "\n",
    "您可能会注意到，我们不会像通常那样在每集开始时重置矢量化环境。这是因为一旦episode结束，每个环境都会自动重置（由于随机种子，每个环境需要不同数量的时间步来完成剧集）。因此，我们也不会收集Episode中的数据，而只是在每个环境中播放一定数量的步骤 ( n_steps_per_update )（例如，这可能意味着我们播放 20 个时间步来完成一个剧集，然后使用其余的时间步）开始新的步骤的时间步骤）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create a wrapper environment to save episode returns and episode lengths\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m envs_wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrappers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordEpisodeStatistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m critic_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m actor_losses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\wrappers\\common.py:496\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.__init__\u001b[1;34m(self, env, buffer_length, stats_key)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This wrapper will keep track of cumulative rewards and episode lengths.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    stats_key: The info key for the episode statistics\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m gym\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRecordConstructorArgs\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 496\u001b[0m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats_key \u001b[38;5;241m=\u001b[39m stats_key\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\core.py:310\u001b[0m, in \u001b[0;36mWrapper.__init__\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wraps an environment to allow a modular transformation of the :meth:`step` and :meth:`reset` methods.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    env: The environment to wrap\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, Env)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space: spaces\u001b[38;5;241m.\u001b[39mSpace[WrapperActType] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observation_space: spaces\u001b[38;5;241m.\u001b[39mSpace[WrapperObsType] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a wrapper environment to save episode returns and episode lengths\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "# Use tqdm to get a progress bar for training\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    # We don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    # Reset lists that collect experiences of an episode (sample phase)\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    # At the start of training reset all envs to get an initial state\n",
    "    if sample_phase == 0:\n",
    "        states, info = envs_wrapper.reset(seed=42)\n",
    "\n",
    "    # Play n steps in our parallel environments to collect data\n",
    "    for step in range(n_steps_per_update):\n",
    "        # Select an action A_{t} using S_{t} as input for the agent\n",
    "        actions, action_log_probs, state_value_preds, entropy = agent.select_action(\n",
    "            states\n",
    "        )\n",
    "\n",
    "        # Perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(\n",
    "            actions.cpu().numpy()\n",
    "        )\n",
    "\n",
    "        ep_value_preds[step] = torch.squeeze(state_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "\n",
    "        # Add a mask (for the return calculation later);\n",
    "        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "    # Calculate the losses for actor and critic\n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        ep_value_preds,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # Update the actor and critic networks\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    # Log the losses and entropy\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "    entropies.append(entropy.detach().mean().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the LunarLander-v3 environment \\n \\\n",
    "             (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={randomize_domain})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"file://_static/img/tutorials/vector_env_a2c_training_plots.png\" alt=\"training_plots\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis of Synchronous and Asynchronous Vectorized Environments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "Asynchronous environments can lead to quicker training times and a higher speedup\n",
    "for data collection compared to synchronous environments. This is because asynchronous environments\n",
    "allow multiple agents to interact with their environments in parallel,\n",
    "while synchronous environments run multiple environments serially.\n",
    "This results in better efficiency and faster training times for asynchronous environments.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"file://_static/img/tutorials/vector_env_performance_plots.png\" alt=\"performance_plots\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "According to the Karp-Flatt metric (a metric used in parallel computing to estimate the limit for the\n",
    "speedup when scaling up the number of parallel processes, here the number of environments),\n",
    "the estimated max. speedup for asynchronous environments is 57, while the estimated maximum speedup\n",
    "for synchronous environments is 21. This suggests that asynchronous environments have significantly\n",
    "faster training times compared to synchronous environments (see graphs).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"file://_static/img/tutorials/vector_env_karp_flatt_plot.png\" alt=\"karp_flatt_metric\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "However, it is important to note that increasing the number of parallel vector environments\n",
    "can lead to slower training times after a certain number of environments (see plot below, where the\n",
    "agent was trained until the mean training returns were above -120). The slower training times might occur\n",
    "because the gradients of the environments are good enough after a relatively low number of environments\n",
    "(especially if the environment is not very complex). In this case, increasing the number of environments\n",
    "does not increase the learning speed, and actually increases the runtime, possibly due to the additional time\n",
    "needed to calculate the gradients. For LunarLander-v3, the best performing configuration used a AsyncVectorEnv\n",
    "with 10 parallel environments, but environments with a higher complexity may require more\n",
    "parallel environments to achieve optimal performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"file://_static/img/tutorials/vector_env_runtime_until_threshold.png\" alt=\"runtime_until_threshold_plot\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/ Loading Weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_weights = False\n",
    "load_weights = False\n",
    "\n",
    "actor_weights_path = \"weights/actor_weights.h5\"\n",
    "critic_weights_path = \"weights/critic_weights.h5\"\n",
    "\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.mkdir(\"weights\")\n",
    "\n",
    "\"\"\" save network weights \"\"\"\n",
    "if save_weights:\n",
    "    torch.save(agent.actor.state_dict(), actor_weights_path)\n",
    "    torch.save(agent.critic.state_dict(), critic_weights_path)\n",
    "\n",
    "\n",
    "\"\"\" load network weights \"\"\"\n",
    "if load_weights:\n",
    "    agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr)\n",
    "\n",
    "    agent.actor.load_state_dict(torch.load(actor_weights_path))\n",
    "    agent.critic.load_state_dict(torch.load(critic_weights_path))\n",
    "    agent.actor.eval()\n",
    "    agent.critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcase the Agent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" play a couple of showcase episodes \"\"\"\n",
    "\n",
    "n_showcase_episodes = 3\n",
    "\n",
    "for episode in range(n_showcase_episodes):\n",
    "    print(f\"starting episode {episode}...\")\n",
    "\n",
    "    # create a new sample environment to get new random parameters\n",
    "    if randomize_domain:\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v3\",\n",
    "            render_mode=\"human\",\n",
    "            gravity=np.clip(\n",
    "                np.random.normal(loc=-10.0, scale=2.0), a_min=-11.99, a_max=-0.01\n",
    "            ),\n",
    "            enable_wind=np.random.choice([True, False]),\n",
    "            wind_power=np.clip(\n",
    "                np.random.normal(loc=15.0, scale=2.0), a_min=0.01, a_max=19.99\n",
    "            ),\n",
    "            turbulence_power=np.clip(\n",
    "                np.random.normal(loc=1.5, scale=1.0), a_min=0.01, a_max=1.99\n",
    "            ),\n",
    "            max_episode_steps=500,\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(\"LunarLander-v3\", render_mode=\"human\", max_episode_steps=500)\n",
    "\n",
    "    # get an initial state\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # play one episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = agent.select_action(state[None, :])\n",
    "\n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "        # update if the environment is done\n",
    "        done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try playing the environment yourself\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from gymnasium.utils.play import play\n",
    "#\n",
    "# play(gym.make('LunarLander-v3', render_mode='rgb_array'),\n",
    "#     keys_to_action={'w': 2, 'a': 1, 'd': 3}, noop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu. \"Asynchronous Methods for Deep Reinforcement Learning\" ICML (2016).\n",
    "\n",
    "[2] J. Schulman, P. Moritz, S. Levine, M. Jordan and P. Abbeel. \"High-dimensional continuous control using generalized advantage estimation.\" ICLR (2016).\n",
    "\n",
    "[3] Gymnasium Documentation: Vector environments. (URL: https://gymnasium.farama.org/api/vector/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
